# ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆï¼šAIã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè·µè¬›åº§2025 ç¬¬1å› å®¿é¡Œ

ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã§ã¯ã€ã€AIã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè·µè¬›åº§2025ã€‘ç¬¬1å›ã®å®¿é¡Œï¼ˆæ¼”ç¿’èª²é¡Œï¼‰ã®å®Ÿæ–½å†…å®¹ã‚’ã¾ã¨ã‚ã¦ã„ã¾ã™ã€‚

## ğŸ™‹ åŸºæœ¬æƒ…å ±

- **Omnicampus ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå**ï¼š`taiga10969`  
- **åå‰**ï¼šå¢—ç”°å¤§æ²³

---

## ğŸ“Œ å®Ÿæ–½å†…å®¹ï¼ˆæ¦‚è¦ï¼‰

### 1. UIã®æ”¹è‰¯
- ã‚¿ã‚¤ãƒˆãƒ«è¡¨ç¤ºã«ã‚¤ãƒ©ã‚¹ãƒˆã‚¢ã‚¤ã‚³ãƒ³ã‚’è¿½åŠ ã€‚
- `ai-engineering_chatbot_icon.png` ã‚’ä½œæˆã—ã€ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æ°åæƒ…å ±ã¨ã¨ã‚‚ã«ä¸€æšã®ç”»åƒã¨ã—ã¦è¡¨ç¤ºã€‚

### 2. ãƒ¢ãƒ‡ãƒ«é¸æŠã®æ”¹è‰¯
- Gemmaãƒ¢ãƒ‡ãƒ«ï¼ˆ`gemma-2b`, `gemma-2-2b-jpn-it`ï¼‰ã‚’UIä¸Šã§é¸æŠå¯èƒ½ã«ã€‚
- ãƒ¢ãƒ‡ãƒ«æœªé¸æŠæ™‚ã¯ãƒ­ãƒ¼ãƒ‰ã•ã‚Œãªã„ã‚ˆã†ã«åˆæœŸå€¤ã‚’ã€Œãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€ã«è¨­å®šã€‚

### 3. ãƒ¬ãƒãƒ¼ãƒˆãƒšãƒ¼ã‚¸ã®è¿½åŠ 
- ã‚¢ãƒ—ãƒªã«ã€Œãƒ¬ãƒãƒ¼ãƒˆã€ãƒšãƒ¼ã‚¸ã‚’è¿½åŠ ã—ã€å®Ÿè£…å†…å®¹ã‚’Streamlitä¸Šã«è¡¨ç¤ºå¯èƒ½ã«ã€‚
  â€»ãƒ¬ãƒãƒ¼ãƒˆã®å†…å®¹ã¯ã“ã®READMEã®å†…å®¹ã¨åŒæ§˜ï¼



## âš™ï¸ å®Ÿè£…æ–¹æ³•

### 1. UIã®æ”¹è‰¯
`app.py` ã«ä»¥ä¸‹ã®ã‚ˆã†ã«ç”»åƒè¡¨ç¤ºå‡¦ç†ã‚’è¿½åŠ ï¼š

```python
st.image("ai-engineering_chatbot_icon.png", width=1000)
```



### 2. ãƒ¢ãƒ‡ãƒ«é¸æŠã®æ”¹è‰¯

#### ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã®ãƒ¢ãƒ‡ãƒ«é¸æŠå‡¦ç†ï¼ˆ`app.py`ï¼‰

```python
model_options = [
    "ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
    "google/gemma-2b", 
    "google/gemma-2-2b-jpn-it",
]

if "selected_model" not in st.session_state:
    st.session_state.selected_model = "ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"

selected_model = st.sidebar.selectbox(
    "ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
    model_options,
    index=model_options.index(st.session_state.selected_model) if st.session_state.selected_model in model_options else 0,
    on_change=lambda: st.session_state.update(selected_model=st.session_state.selected_model_selector),
    key="selected_model_selector"
)
```

#### ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°ï¼ˆ`app.py`ï¼‰

```python
@st.cache_resource
def load_model(model_name):
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        st.info(f"Using device: {device}")
        pipe = pipeline(
            "text-generation",
            model=model_name,
            model_kwargs={"torch_dtype": torch.bfloat16},
            device=device
        )
        st.success(f"ãƒ¢ãƒ‡ãƒ« '{model_name}' ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        return pipe
    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ« '{model_name}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

pipe = load_model(st.session_state.selected_model)
```

#### ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹å…¥åŠ›å½¢å¼ã®åˆ†å²ï¼ˆ`llm.py`ï¼‰

```python
def generate_response(pipe, user_question):
    if pipe is None:
        return "ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã€‚", 0

    try:
        if st.session_state.selected_model == "google/gemma-2b":
            inputs = user_question
        elif st.session_state.selected_model == "google/gemma-2-2b-jpn-it":
            inputs = [{"role": "user", "content": user_question}]
        
        outputs = pipe(inputs, max_new_tokens=512, do_sample=True, temperature=0.7, top_p=0.9)
        # å‡ºåŠ›å‡¦ç†ã¯ãƒ¢ãƒ‡ãƒ«ä»•æ§˜ã«å¿œã˜ã¦èª¿æ•´
        ...
```



### 3. ãƒ¬ãƒãƒ¼ãƒˆãƒšãƒ¼ã‚¸ã®è¿½åŠ 

#### `app.py` å†…ã§ã®ãƒšãƒ¼ã‚¸åˆ†å²è¿½åŠ ï¼š

```python
elif st.session_state.page == "ãƒ¬ãƒãƒ¼ãƒˆ":
    ui.display_report_page()
```

#### `ui.py` å†…ã« `display_report_page()` ã‚’å®šç¾©ã—ã€æœ¬æ–‡è¡¨ç¤ºã‚’å®Ÿè£…ã€‚



## ğŸ¤— ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›çµæœã®æ¯”è¼ƒ

### è³ªå•ï¼šæ¡ƒå¤ªéƒã«ã¤ã„ã¦ãŠã—ãˆã¦ãã ã•ã„

#### `gemma-2b` ã®å‡ºåŠ›ï¼ˆç¹°ã‚Šè¿”ã—æ–‡ï¼‰ï¼š

```
æ¡ƒå¤ªéƒã¯ã€æ¡ƒã‚’100å€‹ã‚‚é£Ÿã¹ã‚‹ã“ã¨ãŒã§ããŸã€‚ï¼ˆä»¥ä¸‹ç¹°ã‚Šè¿”ã—ï¼‰
```

#### `gemma-2-2b-jpn-it` ã®å‡ºåŠ›ï¼ˆæ§‹é€ åŒ–ã•ã‚ŒãŸå†…å®¹ï¼‰ï¼š

```
#æ¡ƒå¤ªéƒã«ã¤ã„ã¦
æ¡ƒå¤ªéƒã¯ã€æ—¥æœ¬ã®æ°‘é–“ä¼æ‰¿ã§ã‚ã‚Šã€å­ä¾›ãŸã¡ã«è¦ªã—ã¿ã‚„ã™ã„ç‰©èªã§ã™ã€‚

##ã‚ã‚‰ã™ã˜
- æ¡ƒã®æ¨¹ã¨ã€æ¡ƒå¤ªéƒã®èª•ç”Ÿ
- æ‚ªé¬¼ã®è¥²æ’ƒ
- æ¡ƒå¤ªéƒã®æ´»èº
- å‹åˆ©ã¨ä¼èª¬

##ç‰¹å¾´
- å­ä¾›å‘ã‘ã€å‹‡æ•¢ã•ã€æ­£ç¾©æ„Ÿã€ä¼çµ±
- ç¾ä»£ã§ã‚‚äººæ°—

##æ–‡åŒ–çš„ãªå½±éŸ¿
- çµµç”»ã€ã‚¢ãƒ‹ãƒ¡ã€ç¥­ã‚Šãªã©ã«æ´¾ç”Ÿ
```

### è€ƒå¯Ÿ
- `gemma-2-2b-jpn-it` ãƒ¢ãƒ‡ãƒ«ã¯æ—¥æœ¬èªã«ç‰¹åŒ–ã—ã¦ã„ã‚‹ãŸã‚ã€æ§‹é€ åŒ–ã•ã‚ŒãŸæ­£ç¢ºãªå›ç­”ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚
- `gemma-2b` ãƒ¢ãƒ‡ãƒ«ã¯æ±ç”¨æ€§ã¯ã‚ã‚‹ãŒã€æ—¥æœ¬èªå¯¾å¿œã®ç²¾åº¦ã«åŠ£ã‚‹ãŸã‚ç¹°ã‚Šè¿”ã—æ–‡ãªã©ãŒå‡ºåŠ›ã•ã‚Œã‚„ã™ã„ã€‚


## ğŸ“ è£œè¶³

- æœ¬ãƒ¬ãƒãƒ¼ãƒˆãƒšãƒ¼ã‚¸ã¯ Streamlit ã‚¢ãƒ—ãƒªå†…ã«è¿½åŠ ã•ã‚Œã¦ãŠã‚Šã€UIã‹ã‚‰é¸æŠã—ã¦ç¢ºèªå¯èƒ½ã§ã™ã€‚
- ãƒ¢ãƒ‡ãƒ«è¿½åŠ ã‚„æ‹¡å¼µã‚‚ä»Šå¾Œç°¡å˜ã«è¡Œãˆã‚‹è¨­è¨ˆã«ãªã£ã¦ã„ã¾ã™ã€‚

---

## å®Ÿè¡Œçµæœ
å®Ÿè¡Œã—ãŸã¨ãã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’æ·»ä»˜ã—ã¾ã™ã€‚
![](https://raw.githubusercontent.com/Taiga10969/lecture-ai-engineering/refs/heads/master/day1/02_streamlit_app/results_image/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202025-04-19%200.58.02.png)
![](https://raw.githubusercontent.com/Taiga10969/lecture-ai-engineering/refs/heads/master/day1/02_streamlit_app/results_image/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202025-04-19%201.23.17.png)
![](https://raw.githubusercontent.com/Taiga10969/lecture-ai-engineering/refs/heads/master/day1/02_streamlit_app/results_image/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202025-04-19%201.23.58.png)

Omnicampus ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåï¼š`taiga10969`  
åå‰ï¼šå¢—ç”°å¤§æ²³
